{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xqfzCWto0hzl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data must be accesible from that directory\n",
    "\n",
    "import pandas as pd\n",
    "data = []\n",
    "\n",
    "with open('hpc/train_GPT', 'r') as f:\n",
    "    for line in f:\n",
    "        value = line.strip()\n",
    "        data.append(value)\n",
    "\n",
    "# Convert to DataFrame\n",
    "sys20_full = pd.DataFrame(data, columns=[\"EventId\"])\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('bgl/train_GPT', 'r') as f:\n",
    "    for line in f:\n",
    "        value = line.strip()\n",
    "        data.append(value)\n",
    "\n",
    "# Convert to DataFrame\n",
    "bgl_df = pd.DataFrame(data, columns=[\"EventId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jYqx7WpeWsXM"
   },
   "outputs": [],
   "source": [
    "def processBGL(bgl_df):\n",
    "  bgl_df[\"Abnormal\"] = bgl_df[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "  bgl_df['Abnormal-EventId'] = bgl_df['Abnormal'].astype(str) + \"-\" + bgl_df['EventId']\n",
    "  vocab = {id: idx + 1 for idx, id in enumerate(bgl_df['Abnormal-EventId'].unique())}\n",
    "  vocab['<PAD>'] = 0\n",
    "  reverse_vocab = {index: label for label, index in vocab.items()}\n",
    "  bgl_df['Tokenized'] = bgl_df['Abnormal-EventId'].map(vocab)\n",
    "\n",
    "  # Check for tokens starting with '1'\n",
    "  error_tokens = [vocab[token] for token in vocab.keys() if token.startswith('1')]\n",
    "\n",
    "  return bgl_df, vocab, reverse_vocab, error_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBGL(bgl_df):\n",
    "  vocab = {id: idx + 1 for idx, id in enumerate(bgl_df['EventId'].unique())}\n",
    "  vocab['<PAD>'] = 0\n",
    "  reverse_vocab = {index: label for label, index in vocab.items()}\n",
    "  bgl_df['Tokenized'] = bgl_df['EventId'].map(vocab)\n",
    "\n",
    "  # Check for tokens starting with '1'\n",
    "  error_tokens = [vocab[token] for token in vocab.keys() if token.startswith('1')]\n",
    "\n",
    "  return bgl_df, vocab, reverse_vocab, error_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fmY6OHi0VQ7f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def processHPC(df):\n",
    "  error_events = [\n",
    "    'e7ede03f',\n",
    "    'e07f96ea',\n",
    "    '3dff51fb',\n",
    "    'bd82b62d',\n",
    "    '562e2cee',\n",
    "    'b02b9741',\n",
    "    '74e8333a',\n",
    "    '252f035f',\n",
    "    '683ade8b',\n",
    "    '7e85bcb6',\n",
    "    '5cdfd2f3',\n",
    "    '7b80ea70',\n",
    "    '0cc39f34',\n",
    "    'c0fcff27',\n",
    "    'a84ecf37',\n",
    "    '20bdc066',\n",
    "    'b38fa951',\n",
    "    '8c1777d1',\n",
    "    'dfc857c8',\n",
    "    '6eff2f7f',\n",
    "    '03b9852a'\n",
    "    ]\n",
    "\n",
    "  # Assuming df['EventId'] is already defined in your DataFrame 'df'\n",
    "  # Start the enumeration from 1, reserving 0 for padding\n",
    "  vocab = {id: idx + 1 for idx, id in enumerate(df['EventId'].unique())}\n",
    "\n",
    "  # Adding padding token to the vocabulary\n",
    "  vocab['<PAD>'] = 0\n",
    "\n",
    "  # Creating a reverse vocabulary\n",
    "  reverse_vocab = {index: label for label, index in vocab.items()}\n",
    "\n",
    "  # Mapping the EventId to their respective indices in the vocabulary\n",
    "  df['Tokenized'] = df['EventId'].map(vocab)\n",
    "\n",
    "  error_tokens = [vocab[event] for event in error_events if event in vocab]\n",
    "\n",
    "  return df, vocab, reverse_vocab, error_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D6V5V_2YWtTE"
   },
   "outputs": [],
   "source": [
    "#df, vocab, reverse_vocab, error_tokens = processBGL(bgl_df)\n",
    "df, vocab, reverse_vocab, error_tokens = processHPC(sys20_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "J0kDzMciB7Lr"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from ast import literal_eval\n",
    "\n",
    "\"\"\"\n",
    "MinGPT code\n",
    "Source: https://github.com/karpathy/minGPT/blob/master/mingpt\n",
    "\"\"\"\n",
    "\n",
    "class CfgNode:\n",
    "    \"\"\" a lightweight configuration class inspired by yacs \"\"\"\n",
    "    # TODO: convert to subclass from a dict like in yacs?\n",
    "    # TODO: implement freezing to prevent shooting of own foot\n",
    "    # TODO: additional existence/override checks when reading/writing params?\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._str_helper(0)\n",
    "\n",
    "    def _str_helper(self, indent):\n",
    "        \"\"\" need to have a helper to support nested indentation for pretty printing \"\"\"\n",
    "        parts = []\n",
    "        for k, v in self.__dict__.items():\n",
    "            if isinstance(v, CfgNode):\n",
    "                parts.append(\"%s:\\n\" % k)\n",
    "                parts.append(v._str_helper(indent + 1))\n",
    "            else:\n",
    "                parts.append(\"%s: %s\\n\" % (k, v))\n",
    "        parts = [' ' * (indent * 4) + p for p in parts]\n",
    "        return \"\".join(parts)\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\" return a dict representation of the config \"\"\"\n",
    "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
    "\n",
    "    def merge_from_dict(self, d):\n",
    "        self.__dict__.update(d)\n",
    "\n",
    "    def merge_from_args(self, args):\n",
    "        \"\"\"\n",
    "        update the configuration from a list of strings that is expected\n",
    "        to come from the command line, i.e. sys.argv[1:].\n",
    "\n",
    "        The arguments are expected to be in the form of `--arg=value`, and\n",
    "        the arg can use . to denote nested sub-attributes. Example:\n",
    "\n",
    "        --model.n_layer=10 --trainer.batch_size=32\n",
    "        \"\"\"\n",
    "        for arg in args:\n",
    "\n",
    "            keyval = arg.split('=')\n",
    "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
    "            key, val = keyval # unpack\n",
    "\n",
    "            # first translate val into a python object\n",
    "            try:\n",
    "                val = literal_eval(val)\n",
    "                \"\"\"\n",
    "                need some explanation here.\n",
    "                - if val is simply a string, literal_eval will throw a ValueError\n",
    "                - if val represents a thing (like an 3, 3.14, [1,2,3], False, None, etc.) it will get created\n",
    "                \"\"\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "            # find the appropriate object to insert the attribute into\n",
    "            assert key[:2] == '--'\n",
    "            key = key[2:] # strip the '--'\n",
    "            keys = key.split('.')\n",
    "            obj = self\n",
    "            for k in keys[:-1]:\n",
    "                obj = getattr(obj, k)\n",
    "            leaf_key = keys[-1]\n",
    "\n",
    "            # ensure that this attribute exists\n",
    "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
    "\n",
    "            # overwrite the attribute\n",
    "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
    "            setattr(obj, leaf_key, val)\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "'''\n",
    "#Additive Attention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "        # Additive attention parameters\n",
    "        self.W1 = nn.Linear(config.n_embd // config.n_head, config.n_embd // config.n_head, bias=False)\n",
    "        self.W2 = nn.Linear(config.n_embd // config.n_head, config.n_embd // config.n_head, bias=False)\n",
    "        self.V = nn.Linear(config.n_embd // config.n_head, 1, bias=False)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "\n",
    "        # additive attention score computation\n",
    "        q_proj = self.W1(q)\n",
    "        k_proj = self.W2(k)\n",
    "        att = self.V(torch.tanh(q_proj.unsqueeze(-2) + k_proj.unsqueeze(-3))).squeeze(-1)\n",
    "\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "'''\n",
    "\n",
    "class HierarchicalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_attention = CausalSelfAttention(config)\n",
    "        self.sentence_attention = CausalSelfAttention(config)\n",
    "        self.ln = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying word-level attention\n",
    "        word_level_output = self.word_attention(x)\n",
    "        word_level_output = self.ln(word_level_output)\n",
    "\n",
    "        # Reshaping for sentence-level attention\n",
    "        B, T, C = word_level_output.size()\n",
    "        sentence_input = word_level_output.view(B, -1, T, C).mean(dim=2)\n",
    "\n",
    "        # Applying sentence-level attention\n",
    "        sentence_level_output = self.sentence_attention(sentence_input)\n",
    "\n",
    "        # Combining outputs\n",
    "        combined_output = word_level_output + sentence_level_output.unsqueeze(2).repeat(1, 1, T, 1).view(B, T, C)\n",
    "        return combined_output\n",
    "\n",
    "#For Causal Self Attention\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "        # Two-layer feed-forward network with ReLU activation\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # x = x + self.mlpf(self.ln_2(x))\n",
    "        x = x + self.dropout(self.ffn(self.ln_2(x)))\n",
    "        return x\n",
    "\n",
    "'''\n",
    "#For Hierarchical attention\n",
    "class Block(nn.Module):\n",
    "    \"\"\" A Transformer block with hierarchical attention \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = HierarchicalAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.dropout(self.ffn(self.ln_2(x)))\n",
    "        return x\n",
    "'''\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  \"\"\"\n",
    "  Uses sine and cosine functions to create positional encodings, which will be added to the log event embeddings\n",
    "  Params:\n",
    "  - d_model: dimension size\n",
    "  - max_len: block size\n",
    "  \"\"\"\n",
    "  def __init__(self, d_model, max_len):\n",
    "      super(PositionalEncoding, self).__init__()\n",
    "      pe = torch.zeros(max_len, d_model)\n",
    "      # matrix of position indices (max_len, 1)\n",
    "      position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "      # computing positional encodings\n",
    "      # div_term = torch.pow(10000, torch.arange(0, d_model, 2, dtype=torch.float, device=device) / d_model)\n",
    "      div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "      pe[:, 0::2] = torch.sin(position * div_term) # even positions in input vectors of log events in sequence\n",
    "      pe[:, 1::2] = torch.cos(position * div_term) # odd positions in input vectors of log events in sequence\n",
    "      pe = pe.unsqueeze(0)\n",
    "      self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.pe[:, :x.size(1)]\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\" GPT Language Model \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CfgNode()\n",
    "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = None\n",
    "        C.n_head = None\n",
    "        C.n_embd =  None\n",
    "        # these options must be filled in externally\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        # dropout hyperparameters\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        assert config.input_size is not None\n",
    "        assert config.target_size is not None\n",
    "\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.input_size = config.input_size\n",
    "        self.target_size = config.target_size\n",
    "\n",
    "        type_given = config.model_type is not None\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        if type_given:\n",
    "            # translate from model_type to detailed configuration\n",
    "            config.merge_from_dict({\n",
    "                # names follow the huggingface naming conventions\n",
    "                # GPT-1\n",
    "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
    "                # GPT-2 configs\n",
    "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "                # Gophers\n",
    "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "                # (there are a number more...)\n",
    "                # I made these tiny models up\n",
    "                'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
    "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "            }[config.model_type])\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = PositionalEncoding(config.n_embd, config.block_size),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def from_pretrained(self, model):\n",
    "        self.block_size = model.block_size\n",
    "        self.input_size = model.input_size\n",
    "        self.target_size = model.target_size\n",
    "\n",
    "        self.transformer = model.transformer\n",
    "        self.lm_head = model.lm_head\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None, use_softmax = False):\n",
    "        device = idx.device\n",
    "        if use_softmax:\n",
    "            b, t, _ = idx.size()\n",
    "        else:\n",
    "            b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "\n",
    "        # pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0).unsqueeze(-1)  # shape (1, t, 1)\n",
    "\n",
    "        if use_softmax:\n",
    "            # Assuming idx is the softmax distribution here\n",
    "            tok_emb = torch.matmul(idx, self.transformer.wte.weight)  # token embeddings of shape (b, t, n_embd)\n",
    "        else:\n",
    "            tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def predict_next_event(self, idx, max_new_tokens, sequence_length=1, temperature=1.0, use_softmax= False):\n",
    "      \"\"\"\n",
    "      Predict the next event given a sequence of indices idx.\n",
    "      \"\"\"\n",
    "      for _ in range(max_new_tokens):\n",
    "          # forward pass to get the logits\n",
    "          logits, _ = self(idx, use_softmax = use_softmax)\n",
    "          # applying softmax to get probabilities\n",
    "          probs = F.softmax(logits[:, -sequence_length, :] / temperature, dim=-1)\n",
    "          next_event_idx = probs\n",
    "          if not use_softmax:\n",
    "              # index of the event with the highest probability\n",
    "              next_event_idx = torch.argmax(probs, dim=-1)\n",
    "          # appending the predicted event index to the input sequence\n",
    "          idx = torch.cat((idx, next_event_idx.unsqueeze(1)), dim=1)\n",
    "          # removing the first element to maintain a constant sequence length\n",
    "          idx = idx[:, 1:]\n",
    "\n",
    "      return idx\n",
    "\n",
    "    def predict_log_sequence(self, idx, sequence_length, vocabulary, predict_next_sequence_length = 1, use_softmax= False):\n",
    "        \"\"\"\n",
    "        Predict a log sequence of the desired length.\n",
    "        \"\"\"\n",
    "\n",
    "        if predict_next_sequence_length <= 1:\n",
    "          temperature = 1.0\n",
    "\n",
    "          logits, _ = self(idx)\n",
    "\n",
    "          last_logits = logits[:, -sequence_length:, :]\n",
    "\n",
    "          probs = F.softmax(last_logits / temperature, dim=-1)\n",
    "\n",
    "          next_event_idx = torch.argmax(probs, dim=-1)\n",
    "\n",
    "          return next_event_idx\n",
    "\n",
    "        else:\n",
    "          predicted_sequence = idx.clone()\n",
    "          if use_softmax:\n",
    "            vocabulary_size = len(vocabulary)\n",
    "\n",
    "            # Create a one-hot encoding tensor\n",
    "            b, t = predicted_sequence.shape\n",
    "            prob = torch.zeros(b, t, vocabulary_size, device=predicted_sequence.device)\n",
    "            prob.scatter_(2, predicted_sequence.unsqueeze(-1), 1)\n",
    "\n",
    "            predicted_sequence = prob\n",
    "\n",
    "          \n",
    "\n",
    "          for i in range(predict_next_sequence_length):\n",
    "            #print(i, \": \",predicted_sequence[0])\n",
    "            # predicting the next event\n",
    "            predicted_sequence = self.predict_next_event(predicted_sequence, 1, use_softmax = use_softmax)\n",
    "\n",
    "          actaul_predictions = predicted_sequence[:, -predict_next_sequence_length * sequence_length:]\n",
    "\n",
    "          return predicted_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X6VgTNQqVbWN"
   },
   "outputs": [],
   "source": [
    "#GPT Dataset preperation\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import numpy as np\n",
    "\n",
    "class EventLogDataset(Dataset):\n",
    "    def __init__(self, dataframe, input_size, target_size, block_size, split = \"train\"):\n",
    "        assert block_size >= input_size\n",
    "        assert block_size >= target_size\n",
    "\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.target_size = target_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Ensure each item in the dataframe['Tokenized'] is a tensor of fixed length\n",
    "        self.full_data = dataframe['Tokenized'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == \"train\" :\n",
    "          return len(self.full_data) - self.block_size\n",
    "        else:\n",
    "          return len(self.full_data) - self.input_size - self.target_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.array(self.full_data)\n",
    "        cat = None\n",
    "        if self.split == \"train\" :\n",
    "          cat = arr[idx:idx+self.input_size + 1]\n",
    "        else:\n",
    "          cat = arr[idx:idx+self.input_size + self.target_size]\n",
    "\n",
    "        input_seq = None\n",
    "        input_seq = torch.tensor(cat[:self.block_size], dtype=torch.long).clone()\n",
    "        target_seq = torch.tensor(cat[-self.block_size:], dtype=torch.long).clone()\n",
    "\n",
    "        # We only want to predict at output locations, mask out the loss at the input locations\n",
    "        if self.split == \"test\":\n",
    "          input_seq[self.input_size:] = 0\n",
    "        if self.split == \"train\":\n",
    "          target_seq[:- 1] = -1\n",
    "\n",
    "        return input_seq, target_seq  # Return the input and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VSXoQstA131X"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CfgNode()\n",
    "        # device to train on\n",
    "        C.device = 'auto'\n",
    "        # dataloder parameters\n",
    "        C.num_workers = 4\n",
    "        # optimizer parameters\n",
    "        C.max_iters = None\n",
    "        C.batch_size = 64\n",
    "        C.learning_rate = 3e-4\n",
    "        C.betas = (0.9, 0.95)\n",
    "        C.weight_decay = 0.1 # only applied on matmul weights\n",
    "        C.grad_norm_clip = 1.0\n",
    "        C.num_epochs = 10\n",
    "        C.validation_interval = 500\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, model, train_dataset, val_dataset):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.predictions = None\n",
    "        self.y = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "        self.num_epochs = config.num_epochs\n",
    "\n",
    "        # determine the device we'll train on\n",
    "        if config.device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(\"running on device\", self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_correct = 0\n",
    "        total_loss = 0\n",
    "        total_examples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                logits, loss = self.model(x, y)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                # Mask to ignore -1 values\n",
    "                mask = y != -1\n",
    "\n",
    "                # Apply the mask to the predictions and the true labels\n",
    "                masked_predictions = predictions[:, -1]\n",
    "                masked_labels = y[:, -1]\n",
    "\n",
    "                # Calculate the number of correct predictions\n",
    "                correct = (masked_predictions == masked_labels).sum().item()\n",
    "\n",
    "                total_correct += correct\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_examples += x.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_examples\n",
    "        avg_accuracy = total_correct / total_examples\n",
    "\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "\n",
    "        # setup the dataloader\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = time.time()\n",
    "        losses = []\n",
    "\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            start_time = time.time()\n",
    "            total_correct = 0\n",
    "            total_examples = 0\n",
    "            total_loss = 0\n",
    "            # training\n",
    "\n",
    "            # Let's say you want to use only the first 100 batches from the train_loader\n",
    "            num_batches = self.config.max_iters\n",
    "            \n",
    "            # Use itertools.islice to slice the iterator\n",
    "            sliced_train_loader = itertools.islice(train_loader, num_batches)\n",
    "            \n",
    "            # Enumerate the sliced iterator\n",
    "            progress_bar = tqdm(enumerate(sliced_train_loader), total=num_batches, desc=f'Epoch {epoch + 1}/{self.num_epochs}')\n",
    "\n",
    "            for i, batch in progress_bar:\n",
    "              x, y = batch\n",
    "              x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "\n",
    "              logits, self.loss = model(x, y)\n",
    "              model.zero_grad(set_to_none=True)\n",
    "              self.loss.backward()\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "              self.optimizer.step()\n",
    "\n",
    "\n",
    "              self.iter_num += 1\n",
    "\n",
    "              predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "              # Mask to ignore -1 values\n",
    "              mask = y != -1\n",
    "\n",
    "                # Apply the mask to the predictions and the true labels\n",
    "              masked_predictions = predictions[:,-1]\n",
    "              masked_labels = y[:,-1]\n",
    "\n",
    "              # Calculate the number of correct predictions\n",
    "              correct = (masked_predictions == masked_labels).sum().item()\n",
    "\n",
    "\n",
    "              total_correct += correct\n",
    "              total_examples += x.size(0)\n",
    "\n",
    "              if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=self.loss.item(), accuracy=float(correct) / x.size(0))\n",
    "\n",
    "              total_loss += self.loss.item() * x.size(0)\n",
    "\n",
    "              if self.iter_num % config.validation_interval == 0:\n",
    "                train_loss = total_loss / total_examples\n",
    "                train_accuracy = total_correct / total_examples\n",
    "\n",
    "                val_loss, val_accuracy = self.evaluate(val_loader)\n",
    "\n",
    "                print(f\"Step {self.iter_num}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, \"\n",
    "                      f\"Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n",
    "                model.train()\n",
    "\n",
    "\n",
    "            train_loss = total_loss / total_examples\n",
    "            losses.append(train_loss)\n",
    "            train_accuracy = total_correct / total_examples\n",
    "\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            #variables for reporting\n",
    "            self.epoch = epoch\n",
    "            self.train_loss = train_loss\n",
    "            self.train_accuracy = train_accuracy\n",
    "            self.epoch_time = epoch_time\n",
    "\n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_adjacent_values(df, block_size):\n",
    "    filtered_rows = []\n",
    "    current_value = None\n",
    "    count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        value = row[\"Tokenized\"]\n",
    "        if value == current_value:\n",
    "            count += 1\n",
    "        else:\n",
    "            current_value = value\n",
    "            count = 1\n",
    "        \n",
    "        if count <= block_size + 1:\n",
    "            filtered_rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(filtered_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventLogDataset(Dataset):\n",
    "    def __init__(self, dataframes, input_size, target_size, block_size, split=\"train\"):\n",
    "        assert block_size >= input_size\n",
    "        assert block_size >= target_size\n",
    "\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.target_size = target_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Store the dataframes\n",
    "        self.dataframes = dataframes\n",
    "        self.full_data = [df['Tokenized'].values for df in dataframes]\n",
    "\n",
    "    def __len__(self):\n",
    "        total_length = 0\n",
    "        for data in self.full_data:\n",
    "            if self.split == \"train\":\n",
    "                total_length += len(data) - self.block_size\n",
    "            else:\n",
    "                total_length += len(data) - self.input_size - self.target_size\n",
    "        return total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which dataframe the idx falls into\n",
    "        current_idx = idx\n",
    "        for data in self.full_data:\n",
    "            length = len(data) - (self.block_size if self.split == \"train\" else self.input_size + self.target_size)\n",
    "            if current_idx < length:\n",
    "                arr = np.array(data)\n",
    "                cat = None\n",
    "                if self.split == \"train\":\n",
    "                    cat = arr[current_idx:current_idx+self.input_size + 1]\n",
    "                else:\n",
    "                    cat = arr[current_idx:current_idx+self.input_size + self.target_size]\n",
    "\n",
    "                input_seq = torch.tensor(cat[:self.block_size], dtype=torch.long).clone()\n",
    "                target_seq = torch.tensor(cat[-self.block_size:], dtype=torch.long).clone()\n",
    "\n",
    "                # We only want to predict at output locations, mask out the loss at the input locations\n",
    "                if self.split == \"test\":\n",
    "                    input_seq[self.input_size:] = 0\n",
    "                if self.split == \"train\":\n",
    "                    target_seq[:-1] = -1\n",
    "\n",
    "                return input_seq, target_seq  # Return the input and target sequences\n",
    "            else:\n",
    "                current_idx -= length\n",
    "\n",
    "        raise IndexError(\"Index out of range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1717136432760,
     "user": {
      "displayName": "Halil Ozgur Demir",
      "userId": "08115133693153638537"
     },
     "user_tz": 420
    },
    "id": "CX00N30N1ZsE",
    "outputId": "cb5a2076-c66f-4f06-fd1e-44d3de5fc6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 260436 validation set: 19140\n",
      "number of parameters: 113.49M\n"
     ]
    }
   ],
   "source": [
    "#Data Preperation\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "GPTconfig = GPT.get_default_config();\n",
    "GPTconfig.model_type = 'gpt2'\n",
    "GPTconfig.vocab_size = len(vocab)\n",
    "GPTconfig.input_size = 512 # number of log events in a sequence\n",
    "GPTconfig.target_size = 512\n",
    "GPTconfig.block_size = 512\n",
    "\n",
    "filtered_train_df = filter_adjacent_values(df, GPTconfig.block_size)\n",
    "\n",
    "np.random.seed(55)\n",
    "\n",
    "# Assuming 'df' is your DataFrame containing the tokenized data\n",
    "val_size = int(len(filtered_train_df) * 0.01)\n",
    "validation_indices = np.random.choice(len(filtered_train_df) - val_size, 10, replace=False)\n",
    "validation_dfs = [filtered_train_df.iloc[idx:idx + val_size] for idx in validation_indices]\n",
    "\n",
    "# Get the training data excluding validation indices\n",
    "train_dfs = []\n",
    "start_idx = 0\n",
    "sorted_validation_indices = sorted(validation_indices)\n",
    "\n",
    "for idx in sorted(validation_indices):\n",
    "    if start_idx < idx:\n",
    "        train_dfs.append(filtered_train_df.iloc[start_idx:idx])\n",
    "    start_idx = idx + val_size\n",
    "\n",
    "# Append the remaining part after the last validation index\n",
    "if start_idx < len(filtered_train_df):\n",
    "    train_dfs.append(filtered_train_df.iloc[start_idx:])\n",
    "\n",
    "\n",
    "train_dataset = EventLogDataset(train_dfs, GPTconfig.input_size, GPTconfig.target_size, GPTconfig.block_size)\n",
    "train_validation_dataset = EventLogDataset(validation_dfs, GPTconfig.input_size, GPTconfig.target_size, GPTconfig.block_size)\n",
    "\n",
    "validation_dataset = EventLogDataset(validation_dfs, GPTconfig.input_size, GPTconfig.target_size, GPTconfig.block_size, split=\"test\")\n",
    "print('training set:', len(train_dataset), 'validation set:', len(validation_dataset))\n",
    "\n",
    "#model initialization\n",
    "model = GPT(GPTconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.from_pretrained(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_new = torch.load('bgl_train.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1717136436190,
     "user": {
      "displayName": "Halil Ozgur Demir",
      "userId": "08115133693153638537"
     },
     "user_tz": 420
    },
    "id": "kS3ffChm2Fbf",
    "outputId": "cd35f0a6-9189-44f4-d9d8-07450e9e7440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "#Trainer Initialization\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 0.0001 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 2500\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = 32\n",
    "train_config.num_epochs = 10 #200\n",
    "train_config.validation_interval = 100\n",
    "trainer = Trainer(train_config, model, train_dataset, train_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 946749,
     "status": "ok",
     "timestamp": 1717137382936,
     "user": {
      "displayName": "Halil Ozgur Demir",
      "userId": "08115133693153638537"
     },
     "user_tz": 420
    },
    "id": "9axPMj2K48UT",
    "outputId": "9fd28355-a7dd-4d5e-e578-27a00521efc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hozgu\\anaconda3\\envs\\nanoGPT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Epoch 1/10: 100%|| 2500/2500 [17:42<00:00,  2.35it/s, accuracy=0.531, loss=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.9547, Train Acc: 0.6327, Time: 1062.94 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|| 2500/2500 [17:35<00:00,  2.37it/s, accuracy=0.812, loss=0.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.6658, Train Acc: 0.7650, Time: 1055.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|| 2500/2500 [17:28<00:00,  2.38it/s, accuracy=0.75, loss=0.598] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.6182, Train Acc: 0.7777, Time: 1048.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|| 2500/2500 [17:26<00:00,  2.39it/s, accuracy=0.875, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.5922, Train Acc: 0.7835, Time: 1046.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|| 2500/2500 [17:28<00:00,  2.39it/s, accuracy=0.75, loss=0.73]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.5874, Train Acc: 0.7858, Time: 1048.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 7/2500 [00:03<18:46,  2.21it/s, accuracy=0.875, loss=0.536]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mepoch_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_end_callback)\n\u001b[1;32m----> 4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn[10], line 152\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    150\u001b[0m logits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m model(x, y)\n\u001b[0;32m    151\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    153\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nanoGPT\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nanoGPT\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nanoGPT\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    print(f'Epoch [{trainer.epoch+1}/{trainer.num_epochs}], Train Loss: {trainer.train_loss:.4f}, Train Acc: {trainer.train_accuracy:.4f}, Time: {trainer.epoch_time:.2f} sec')\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "p0A1g1dKRt3J"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evaluate_predictions(model, data_loader, vocabulary, trainer, target_size, unique_error_ids, use_softmax=False):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # For error predictions\n",
    "    true_errors = []\n",
    "    predicted_errors = []\n",
    "\n",
    "    # For error occurrence predictions\n",
    "    true_error_occurrence = []\n",
    "    predicted_error_occurrence = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=\"Evaluating\")\n",
    "        for b, (input_sequence, target_sequence) in enumerate(pbar):\n",
    "            # Move model to CPU\n",
    "            #model.to('cpu')\n",
    "\n",
    "            # Change the device in your trainer configuration to CPU\n",
    "            #trainer.device = torch.device('cpu')\n",
    "\n",
    "            # predictions for the input sequence\n",
    "            input_sequence, target_sequence = input_sequence.to(trainer.device), target_sequence.to(trainer.device)\n",
    "            predicted_sequence = model.predict_log_sequence(input_sequence, 1, vocabulary, target_size, use_softmax=use_softmax)\n",
    "\n",
    "            if use_softmax:\n",
    "              # index of the event with the highest probability\n",
    "              predicted_sequence = torch.argmax(predicted_sequence, dim=-1)\n",
    "\n",
    "\n",
    "            predicted_labels.extend(predicted_sequence)  # Assuming sequence might already be a list or similar\n",
    "\n",
    "            target_sequence = target_sequence[:, -target_size:]\n",
    "            true_labels.extend(target_sequence)\n",
    "\n",
    "            # Check each token if it is an error\n",
    "            batch_true_errors = [1 if label.item() in unique_error_ids else 0 for seq in target_sequence for label in seq.flatten()]\n",
    "            batch_predicted_errors = [1 if label.item() in unique_error_ids else 0 for seq in predicted_sequence for label in seq.flatten()]\n",
    "\n",
    "            true_errors.extend(batch_true_errors)\n",
    "            predicted_errors.extend(batch_predicted_errors)\n",
    "\n",
    "            # Determine if an error occurs in each sequence\n",
    "            batch_true_occurrence = [1 if any(label.item() in unique_error_ids for label in seq) else 0 for seq in target_sequence]\n",
    "            batch_predicted_occurrence = [1 if any(label.item() in unique_error_ids for label in seq) else 0 for seq in predicted_sequence]\n",
    "\n",
    "            true_error_occurrence.extend(batch_true_occurrence)\n",
    "            predicted_error_occurrence.extend(batch_predicted_occurrence)\n",
    "\n",
    "            # Calculate intermediate accuracies\n",
    "            current_true_labels = [token.item() for batch in true_labels for token in batch.flatten()]\n",
    "            current_predicted_labels = [token.item() for batch in predicted_labels for token in batch.flatten()]\n",
    "\n",
    "            current_true_errors = true_errors\n",
    "            current_predicted_errors = predicted_errors\n",
    "\n",
    "            current_true_occurrence = true_error_occurrence\n",
    "            current_predicted_occurrence = predicted_error_occurrence\n",
    "\n",
    "            # General accuracy\n",
    "            accuracy = sum(1 for true, pred in zip(current_true_labels, current_predicted_labels) if true == pred) / len(current_true_labels)\n",
    "\n",
    "            # Error predictions accuracy\n",
    "            error_accuracy = sum(1 for true, pred in zip(current_true_errors, current_predicted_errors) if true == pred) / len(current_true_errors)\n",
    "\n",
    "            # Error occurrence predictions accuracy\n",
    "            occurrence_accuracy = sum(1 for true, pred in zip(current_true_occurrence, current_predicted_occurrence) if true == pred) / len(current_true_occurrence)\n",
    "\n",
    "            # Update progress bar with intermediate accuracies\n",
    "            pbar.set_description(f\"Evaluating (Acc: {accuracy:.4f}, Err Acc: {error_accuracy:.4f}, Occ Acc: {occurrence_accuracy:.4f})\")\n",
    "\n",
    "            if (b + 1) % 30 == 0:\n",
    "\n",
    "\n",
    "                break\n",
    "\n",
    "    true_labels = [token.item() for batch in true_labels for token in batch.flatten()]\n",
    "    #true_labels = [vocabulary[label if label != -1 else 0] for label in true_labels]\n",
    "\n",
    "    predicted_labels = [token.item() for batch in predicted_labels for token in batch.flatten()]\n",
    "    #true_labels = [vocabulary[token.item()] for token in true_labels]\n",
    "    #print(predicted_labels[0], \"   \", true_labels[0])\n",
    "\n",
    "\n",
    "    #true_errors = [token for batch in true_errors for token in batch]\n",
    "    #predicted_errors = [token for batch in predicted_errors for token in batch]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')\n",
    "    accuracy = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred) / len(true_labels)\n",
    "\n",
    "        # Evaluation for error predictions\n",
    "    error_precision, error_recall, error_f1, _ = precision_recall_fscore_support(true_errors, predicted_errors, average='binary')\n",
    "    error_accuracy = sum(1 for true, pred in zip(true_errors, predicted_errors) if true == pred) / len(true_errors)\n",
    "\n",
    "    # Evaluation for error occurrence predictions\n",
    "    occurrence_precision, occurrence_recall, occurrence_f1, _ = precision_recall_fscore_support(true_error_occurrence, predicted_error_occurrence, average='binary')\n",
    "    occurrence_accuracy = sum(1 for true, pred in zip(true_error_occurrence, predicted_error_occurrence) if true == pred) / len(true_error_occurrence)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'error_accuracy': error_accuracy, 'error_precision': error_precision, 'error_recall': error_recall, 'error_f1': error_f1,\n",
    "        'occurrence_accuracy': occurrence_accuracy, 'occurrence_precision': occurrence_precision, 'occurrence_recall': occurrence_recall, 'occurrence_f1': occurrence_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ozBER-7rsRt",
    "outputId": "dfa30712-34cd-44b9-b67f-5af3381ca808"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (Acc: 0.5273, Err Acc: 0.9006, Occ Acc: 0.4562):   5%|         | 29/599 [52:08<17:04:58, 107.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5273, Validation Precision: 0.0966, Recall: 0.0949, F1: 0.0944\n",
      "Error Prediction - Accuracy: 0.9006, Precision: 0.2498, Recall: 0.2128, F1: 0.2298\n",
      "Error Occurrence - Accuracy: 0.4562, Precision: 1.0000, Recall: 0.0984, F1: 0.1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hozgu\\anaconda3\\envs\\nanoGPT\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hozgu\\anaconda3\\envs\\nanoGPT\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=train_config.batch_size, pin_memory=True, shuffle=True, drop_last=False, num_workers=train_config.num_workers,)\n",
    "# Evaluate the predictions (use_softmax = true for probabilistic inference)\n",
    "evaluation_results = evaluate_predictions(model, validation_loader, reverse_vocab, trainer, validation_dataset.target_size, error_tokens, use_softmax = False)\n",
    "\n",
    "# Extract metrics for validation\n",
    "validation_accuracy = evaluation_results['accuracy']\n",
    "validation_precision = evaluation_results['precision']\n",
    "validation_recall = evaluation_results['recall']\n",
    "validation_f1 = evaluation_results['f1']\n",
    "\n",
    "# Additional metrics for error analysis might be of interest\n",
    "error_accuracy = evaluation_results['error_accuracy']\n",
    "error_precision = evaluation_results['error_precision']\n",
    "error_recall = evaluation_results['error_recall']\n",
    "error_f1 = evaluation_results['error_f1']\n",
    "\n",
    "occurrence_accuracy = evaluation_results['occurrence_accuracy']\n",
    "occurrence_precision = evaluation_results['occurrence_precision']\n",
    "occurrence_recall = evaluation_results['occurrence_recall']\n",
    "occurrence_f1 = evaluation_results['occurrence_f1']\n",
    "\n",
    "# Print the standard validation metrics\n",
    "print(f'Validation Accuracy: {validation_accuracy:.4f}, Validation Precision: {validation_precision:.4f}, Recall: {validation_recall:.4f}, F1: {validation_f1:.4f}')\n",
    "\n",
    "# Optionally print the error prediction metrics\n",
    "print(f'Error Prediction - Accuracy: {error_accuracy:.4f}, Precision: {error_precision:.4f}, Recall: {error_recall:.4f}, F1: {error_f1:.4f}')\n",
    "\n",
    "# Optionally print the error occurrence metrics\n",
    "print(f'Error Occurrence - Accuracy: {occurrence_accuracy:.4f}, Precision: {occurrence_precision:.4f}, Recall: {occurrence_recall:.4f}, F1: {occurrence_f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
