{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: llnl_data/BGL.log\n",
      "Total size after encoding is 4713493 4747963\n",
      "Parsing done. [Time taken: 0:16:14.722308]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from logparser import Spell, Drain\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "START = 2\n",
    "\n",
    "data_dir = os.path.expanduser(\"../Loghub-BGL/\")\n",
    "output_dir = os.path.expanduser(\"../Loghub-BGL/processed/\")\n",
    "log_file = \"BGL.log\"\n",
    "\n",
    "\n",
    "# In the first column of the log, \"-\" indicates non-alert messages while others are alert messages.\n",
    "def count_anomaly():\n",
    "    total_size = 0\n",
    "    normal_size = 0\n",
    "    with open(data_dir + log_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            total_size += 1\n",
    "            if line.split(' ',1)[0] == '-':\n",
    "                normal_size += 1\n",
    "    print(\"total size {}, abnormal size {}\".format(total_size, total_size - normal_size))\n",
    "\n",
    "\n",
    "def deeplog_file_generator(filename, df, features):\n",
    "    with open(filename, 'w') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            for val in zip(*row[features]):\n",
    "                f.write(','.join([str(v) for v in val]) + ' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def parse_log(input_dir, output_dir, log_file, parser_type):\n",
    "    log_format = '<Label> <Id> <Date> <Code1> <Time> <Code2> <Component1> <Component2> <Level> <Content>'\n",
    "    regex = [\n",
    "        r'(0x)[0-9a-fA-F]+', #hexadecimal\n",
    "        r'\\d+.\\d+.\\d+.\\d+',\n",
    "        # r'/\\w+( )$'\n",
    "        r'\\d+'\n",
    "    ]\n",
    "    keep_para = False\n",
    "    if parser_type == \"drain\":\n",
    "        # the hyper parameter is set according to http://jmzhu.logpai.com/pub/pjhe_icws2017.pdf\n",
    "        st = 0.3  # Similarity threshold\n",
    "        depth = 3  # Depth of all leaf nodes\n",
    "        parser = Drain.LogParser(log_format, indir=data_dir, outdir=output_dir, depth=depth, st=st, rex=regex, keep_para=keep_para)\n",
    "        parser.parse(log_file)\n",
    "    elif parser_type == \"spell\":\n",
    "        tau = 0.55\n",
    "        parser = Spell.LogParser(indir=data_dir, outdir=output_dir, log_format=log_format, tau=tau, rex=regex, keep_para=keep_para)\n",
    "        parser.parse(log_file)\n",
    "\n",
    "\n",
    "def sliding_window(raw_data, para):\n",
    "    \"\"\"\n",
    "    split logs into sliding windows/session\n",
    "    :param raw_data: dataframe columns=[timestamp, label, eventid, time duration]\n",
    "    :param para:{window_size: seconds, step_size: seconds}\n",
    "    :return: dataframe columns=[eventids, time durations, label]\n",
    "    \"\"\"\n",
    "    log_size = raw_data.shape[0]\n",
    "    label_data, time_data = raw_data.iloc[:, 1], raw_data.iloc[:, 0]\n",
    "    logkey_data, deltaT_data = raw_data.iloc[:, 2], raw_data.iloc[:, 3]\n",
    "    new_data = []\n",
    "    start_end_index_pair = set()\n",
    "\n",
    "    start_time = time_data[0]\n",
    "    end_time = start_time + para[\"window_size\"]\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "\n",
    "    # get the first start, end index, end time\n",
    "    for cur_time in time_data:\n",
    "        if cur_time < end_time:\n",
    "            end_index += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    start_end_index_pair.add(tuple([start_index, end_index]))\n",
    "\n",
    "    # move the start and end index until next sliding window\n",
    "    num_session = 1\n",
    "    while end_index < log_size:\n",
    "        start_time = start_time + para['step_size']\n",
    "        end_time = start_time + para[\"window_size\"]\n",
    "        for i in range(start_index, log_size):\n",
    "            if time_data[i] < start_time:\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        for j in range(end_index, log_size):\n",
    "            if time_data[j] < end_time:\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "        start_index = i\n",
    "        end_index = j\n",
    "\n",
    "        # when start_index == end_index, there is no value in the window\n",
    "        if start_index != end_index:\n",
    "            start_end_index_pair.add(tuple([start_index, end_index]))\n",
    "\n",
    "        num_session += 1\n",
    "        if num_session % 1000 == 0:\n",
    "            print(\"process {} time window\".format(num_session), end='\\r')\n",
    "\n",
    "    for (start_index, end_index) in start_end_index_pair:\n",
    "        dt = deltaT_data[start_index: end_index].values\n",
    "        dt[0] = 0\n",
    "        new_data.append([\n",
    "            time_data[start_index: end_index].values,\n",
    "            max(label_data[start_index:end_index]),\n",
    "            logkey_data[start_index: end_index].values,\n",
    "            dt\n",
    "        ])\n",
    "\n",
    "    assert len(start_end_index_pair) == len(new_data)\n",
    "    print('there are %d instances (sliding windows) in this dataset\\n' % len(start_end_index_pair))\n",
    "    return pd.DataFrame(new_data, columns=raw_data.columns)\n",
    "\n",
    "\n",
    "##########\n",
    "# Parser #\n",
    "#########\n",
    "parse_log(data_dir, output_dir, log_file, 'drain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size 4747963, abnormal size 348460\n",
      "there are 37315 instances (sliding windows) in this dataset\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Label</th>\n",
       "      <th>EventId</th>\n",
       "      <th>deltaT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1135617911, 1135617913, 1135617915, 113561791...</td>\n",
       "      <td>0</td>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>[0.0, 1.601077, 1.581271, 0.577114, 2.089764, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1127138101, 1127138101, 1127138101, 112713810...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4983ff07, 4983ff07, 4983ff07, 4983ff07, 4983f...</td>\n",
       "      <td>[0.0, 0.13094, 0.073317, 0.157466, 0.121008, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1134118610, 1134118610, 1134118610, 113411861...</td>\n",
       "      <td>0</td>\n",
       "      <td>[30b3b946, 8df7ac9e, a450c390, a450c390, 30b3b...</td>\n",
       "      <td>[0.0, 0.109872, 0.024515, 0.034294, 0.035576, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1136277503, 1136277503, 1136277503, 113627750...</td>\n",
       "      <td>0</td>\n",
       "      <td>[30b3b946, 8df7ac9e, 30b3b946, 8df7ac9e, 30b3b...</td>\n",
       "      <td>[0.0, 0.115572, 0.032737, 0.016155, 0.0, 0.112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1120820379]</td>\n",
       "      <td>0</td>\n",
       "      <td>[8a1ae52c]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37310</th>\n",
       "      <td>[1131198515, 1131198541, 1131198624, 113119869...</td>\n",
       "      <td>0</td>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37311</th>\n",
       "      <td>[1131101810, 1131101810, 1131101810, 113110181...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1b700d02, 1b700d02, 1b700d02, 1b700d02, 1b700...</td>\n",
       "      <td>[0.0, 0.014491, 0.021685, 0.014192, 0.015735, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37312</th>\n",
       "      <td>[1134374630, 1134374630, 1134374630, 113437463...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8df7ac9e, 30b3b946, 8df7ac9e, 26c05abc, 26c05...</td>\n",
       "      <td>[0.0, 0.150302, 0.160659, 0.178033, 0.170768, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37313</th>\n",
       "      <td>[1124338339]</td>\n",
       "      <td>0</td>\n",
       "      <td>[ba77ab8e]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37314</th>\n",
       "      <td>[1135629053, 1135629057, 1135629059, 113562905...</td>\n",
       "      <td>0</td>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>[0.0, 4.139957, 1.111798, 0.574947, 6.120021, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37315 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               timestamp  Label  \\\n",
       "0      [1135617911, 1135617913, 1135617915, 113561791...      0   \n",
       "1      [1127138101, 1127138101, 1127138101, 112713810...      0   \n",
       "2      [1134118610, 1134118610, 1134118610, 113411861...      0   \n",
       "3      [1136277503, 1136277503, 1136277503, 113627750...      0   \n",
       "4                                           [1120820379]      0   \n",
       "...                                                  ...    ...   \n",
       "37310  [1131198515, 1131198541, 1131198624, 113119869...      0   \n",
       "37311  [1131101810, 1131101810, 1131101810, 113110181...      0   \n",
       "37312  [1134374630, 1134374630, 1134374630, 113437463...      0   \n",
       "37313                                       [1124338339]      0   \n",
       "37314  [1135629053, 1135629057, 1135629059, 113562905...      0   \n",
       "\n",
       "                                                 EventId  \\\n",
       "0      [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...   \n",
       "1      [4983ff07, 4983ff07, 4983ff07, 4983ff07, 4983f...   \n",
       "2      [30b3b946, 8df7ac9e, a450c390, a450c390, 30b3b...   \n",
       "3      [30b3b946, 8df7ac9e, 30b3b946, 8df7ac9e, 30b3b...   \n",
       "4                                             [8a1ae52c]   \n",
       "...                                                  ...   \n",
       "37310  [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...   \n",
       "37311  [1b700d02, 1b700d02, 1b700d02, 1b700d02, 1b700...   \n",
       "37312  [8df7ac9e, 30b3b946, 8df7ac9e, 26c05abc, 26c05...   \n",
       "37313                                         [ba77ab8e]   \n",
       "37314  [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...   \n",
       "\n",
       "                                                  deltaT  \n",
       "0      [0.0, 1.601077, 1.581271, 0.577114, 2.089764, ...  \n",
       "1      [0.0, 0.13094, 0.073317, 0.157466, 0.121008, 0...  \n",
       "2      [0.0, 0.109872, 0.024515, 0.034294, 0.035576, ...  \n",
       "3      [0.0, 0.115572, 0.032737, 0.016155, 0.0, 0.112...  \n",
       "4                                                  [0.0]  \n",
       "...                                                  ...  \n",
       "37310                          [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "37311  [0.0, 0.014491, 0.021685, 0.014192, 0.015735, ...  \n",
       "37312  [0.0, 0.150302, 0.160659, 0.178033, 0.170768, ...  \n",
       "37313                                              [0.0]  \n",
       "37314  [0.0, 4.139957, 1.111798, 0.574947, 6.120021, ...  \n",
       "\n",
       "[37315 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########\n",
    "# Count #\n",
    "#########\n",
    "count_anomaly()\n",
    "\n",
    "##################\n",
    "# Transformation #\n",
    "##################\n",
    "# mins\n",
    "window_size = 5\n",
    "step_size = 1\n",
    "train_ratio = 0.4\n",
    "\n",
    "df = pd.read_csv(f'{output_dir}{log_file}_structured.csv')\n",
    "\n",
    "# data preprocess\n",
    "df['datetime'] = pd.to_datetime(df['Time'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
    "df[\"Label\"] = df[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "df['timestamp'] = df[\"datetime\"].values.astype(np.int64) // 10 ** 9\n",
    "df['deltaT'] = df['datetime'].diff() / np.timedelta64(1, 's')\n",
    "df['deltaT'].fillna(0)\n",
    "# convert time to UTC timestamp\n",
    "# df['deltaT'] = df['datetime'].apply(lambda t: (t - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s'))\n",
    "\n",
    "# sampling with fixed window\n",
    "# features = [\"EventId\", \"deltaT\"]\n",
    "# target = \"Label\"\n",
    "# deeplog_df = deeplog_df_transfer(df, features, target, \"datetime\", window_size=args.w)\n",
    "# deeplog_df.dropna(subset=[target], inplace=True)\n",
    "\n",
    "# sampling with sliding window\n",
    "deeplog_df = sliding_window(df[[\"timestamp\", \"Label\", \"EventId\", \"deltaT\"]],\n",
    "                            para={\"window_size\": int(window_size)*60, \"step_size\": int(step_size) * 60}\n",
    "                            )\n",
    "deeplog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 13718\n",
      "test normal size 20579\n",
      "test abnormal size 3018\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Train #\n",
    "#########\n",
    "df_normal =deeplog_df[deeplog_df[\"Label\"] == 0]\n",
    "df_normal = df_normal.sample(frac=1, random_state=12).reset_index(drop=True) #shuffle\n",
    "normal_len = len(df_normal)\n",
    "train_len = int(normal_len * train_ratio)\n",
    "\n",
    "train = df_normal[:train_len]\n",
    "# deeplog_file_generator(os.path.join(output_dir,'train'), train, [\"EventId\", \"deltaT\"])\n",
    "deeplog_file_generator(os.path.join(output_dir,'train'), train, [\"EventId\"])\n",
    "\n",
    "print(\"training size {}\".format(train_len))\n",
    "\n",
    "\n",
    "###############\n",
    "# Test Normal #\n",
    "###############\n",
    "test_normal = df_normal[train_len:]\n",
    "deeplog_file_generator(os.path.join(output_dir, 'test_normal'), test_normal, [\"EventId\"])\n",
    "print(\"test normal size {}\".format(normal_len - train_len))\n",
    "\n",
    "del df_normal\n",
    "del train\n",
    "del test_normal\n",
    "gc.collect()\n",
    "\n",
    "#################\n",
    "# Test Abnormal #\n",
    "#################\n",
    "df_abnormal = deeplog_df[deeplog_df[\"Label\"] == 1]\n",
    "#df_abnormal[\"EventId\"] = df_abnormal[\"EventId\"].progress_apply(lambda e: event_index_map[e] if event_index_map.get(e) else UNK)\n",
    "deeplog_file_generator(os.path.join(output_dir,'test_abnormal'), df_abnormal, [\"EventId\"])\n",
    "print('test abnormal size {}'.format(len(df_abnormal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_count_train = train['EventId'].apply(tuple).nunique()\n",
    "key_count_test = deeplog_df['EventId'].apply(tuple).nunique()\n",
    "print(\"Number of total unique log keys:\", key_count_train + key_count_test)\n",
    "print(\"Number of unique log keys in test set:\", key_count_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
